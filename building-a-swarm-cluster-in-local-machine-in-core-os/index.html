<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="lang">
  <head>
    <title data-vue-tag="true">Nehal Hasnayeen - Nehal Hasnayeen</title><meta data-vue-tag="true" charset="utf-8"><meta data-vue-tag="true" name="generator" content="Gridsome v0.6.9"><meta data-vue-tag="true" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="true" data-key="format-detection" name="format-detection" content="telephone=no"><link data-vue-tag="true" rel="icon" type="image/png" sizes="16x16" href="/gh-page-test/assets/static/favicon.ce0531f.cb378d4.png"><link data-vue-tag="true" rel="icon" type="image/png" sizes="32x32" href="/gh-page-test/assets/static/favicon.ac8d93a.cb378d4.png"><link data-vue-tag="true" rel="icon" type="image/png" sizes="96x96" href="/gh-page-test/assets/static/favicon.b9532cc.cb378d4.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/gh-page-test/assets/static/favicon.f22e9f3.cb378d4.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/gh-page-test/assets/static/favicon.62d22cb.cb378d4.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/gh-page-test/assets/static/favicon.1539b60.cb378d4.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/gh-page-test/assets/static/favicon.dc0cdc5.cb378d4.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/gh-page-test/assets/static/favicon.7b22250.cb378d4.png"><noscript data-vue-tag="true" ><style>.g-image--loading{display:none;}</style></noscript><link rel="preload" href="/gh-page-test/assets/css/0.styles.6b5bcfe0.css" as="style"><link rel="preload" href="/gh-page-test/assets/js/app.ba0f1f39.js" as="script"><link rel="preload" href="/gh-page-test/assets/js/page--src--templates--blog-post-vue.32f2e99a.js" as="script"><link rel="prefetch" href="/gh-page-test/assets/js/page--node-modules--gridsome--app--pages--404-vue.ce18feef.js"><link rel="prefetch" href="/gh-page-test/assets/js/page--src--pages--about-vue.71d8f8e8.js"><link rel="prefetch" href="/gh-page-test/assets/js/page--src--pages--blog-vue.c15761c5.js"><link rel="prefetch" href="/gh-page-test/assets/js/page--src--pages--index-vue.8d129c96.js"><link rel="prefetch" href="/gh-page-test/assets/js/page--src--pages--projects-vue.e3c3810d.js"><link rel="stylesheet" href="/gh-page-test/assets/css/0.styles.6b5bcfe0.css">
  </head>
  <body class="bg-indigo-100 font-display overflow-x-hidden" data-vue-tag="class">
    <div data-server-rendered="true" id="app" class="text-gray-800 flex flex-col h-screen max-w-6xl mx-auto" data-v-d5f1e6c0><header class="py-4 flex justify-between"><div class="overflow-hidden h-16"><a href="/gh-page-test/" class="active"><img alt="logo" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 300 300' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0)' width='300' height='300' xlink:href='data:image/svg%2bxml%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAACXBIWXMAAAsTAAALEwEAmpwYAAADn0lEQVR42u2X3UsUURjGd2ZXXS0zySKNCEsr0ezLyLSyIsxK3VlLJcoMNEj7UuyDxBS3rEQjJcpKLN3ZdTMJpIv6A7zptuu6jO66CSxdrek5u%2b/UuJjmpkH2PvDjnJ1z5px5v87MmkwsFovFYrFYLBaLxWKxWCwWi8VisViTS2Kjx0v%2b35yRDJb8hoPmVKYkyCFWK/XrZEtoluiYQ8OFIwrnZGkoLu%2bP9JYtYcdhXwMNXQGbqH/VP%2bYvhYXxacYlUkAoOcaIbHCYbCgjaZK5ge1E60w0Pzjl9mi%2bNnJZkim1pE0yGB4G4jIvv9xV8FRLXrB83Wn9np0Ng8YMsACVHPCPRd7zxaSovuibCnq/%2bYyyu78WH%2brTWvO7h9TD/drdAo/mUJwj7cUD2jW7a6wmp%2bODpWVIM22udBrrvxuUiJIBMZQ1B8BemrMGVIFEcBTYQCOIp2tibjb9zqN7jpBzD4KTdC0diECsBWXisWlPa1AOsKujfkeoXjP9tivqSL3iHC6CsQP5jz9tzH30cb7iGm2ydX8%2bh7FyzK0Vc/ffe282LNUHVoAz4ARRCVwgCbjJQZGgn67ZQQ21p4BHLAuaac0OcsxqcBtsByvBdZBD60g0lhF0OegZYHMPm9F/AeL2ON5IAWdEJK4nggQwCMTDmDQN5SPJwhFdNFVErx5kUtQ9FK1ScJ8c00IRE9F0UORjgZMMrqe1mul%2bPYtuUdRDKGvu0PVa2m/6DrDDeBgjkyPqADLAG2NwzmJQDUp9812jwkkpwJf/C2JX6VPbQZQ4HsAl0EqpKx76GLUi9beQA4Rz1gNx2DbR28VB91yktpPKpocMVMBzyhiRQQ9p7wqQH5QDFJQAjJHI2BawDTSDAkQ9TRgOXovoG5yyG/SKfnbbOz1TrLS5/gCLKF312hRpnEr9eXTAyhTNcMoSUe9LqXSEsdWGe6OpL8osjvpR1IbQekEcgm5DBri8Gei/BYVk6DNkiG8zmzpixe99oB9UgSf6GhvKHkxny6kiJIwpB0UgImDMPGsfXjj8JCoJUeedoAL0wSldSPsd6N8EN0AjaAOvwHl6a5jxsTTVJ7U0xUNPNi7NmuHiA8iujunloJ8FESCL6l8YfxZsNZRANLgA0um3pPQMz%2bRjmf/qfw7xFvjpBO8vN0YmyHCYJeDeufFJrPjfBsa%2b7IusvxUnv5TnHDduNhyephnOABaLxWKxWCwWi8VisVgsFovFYrFYrD/Sd3quS0gVhIbmAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="300" data-src="/gh-page-test/assets/static/logo.6b65613.b0e31d1.svg" data-srcset="/gh-page-test/assets/static/logo.6b65613.b0e31d1.svg 300w" data-sizes="(max-width: 300px) 100vw, 300px" class="h-64 -mt-24 -ml-8 g-image g-image--lazy g-image--loading"><noscript><img src="/gh-page-test/assets/static/logo.6b65613.b0e31d1.svg" class="h-64 -mt-24 -ml-8 g-image g-image--loaded" width="300" alt="logo"></noscript></a></div><nav class="flex items-center"><div><a href="/gh-page-test/about" class="font-semibold mr-12">About</a><a href="/gh-page-test/blog" class="font-semibold mr-12">Blog</a><a href="/gh-page-test/projects" class="font-semibold">Projects</a></div></nav></header><main class="flex-grow pt-8"><div data-v-d5f1e6c0><div class="max-w-4xl container py-8 my-8 text-gray-800 bg-white px-24 py-16 rounded-lg shadow-xl" data-v-d5f1e6c0><div class="text-center pb-4" data-v-d5f1e6c0><span class="border-b border-indigo-600 pb-1" data-v-d5f1e6c0>
          May 29, 2017
        </span></div><h1 class="text-4xl text-indigo-600 pb-4 text-center" data-v-d5f1e6c0>Building a Swarm Cluster in Local Machine with CoreOS</h1><div class="text-lg leading-relaxed blog" data-v-d5f1e6c0><p>So last week I built a cluster on my local machine with docker swarm to get a taste of clustering. I used CoreOS as operating system on all the nodes as CoreOS is a container specific os. CoreOS also has several tools themselves for cluster orchestration. In this post I’ll show you how I made the cluster on vagrant machines.</p>
<p>First of all you should have <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="nofollow noopener noreferrer">VirtualBox</a> or <a href="https://my.vmware.com/web/vmware/downloads" target="_blank" rel="nofollow noopener noreferrer">VMware</a> to run virtual machine on your host machine and you’ll also need <a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="nofollow noopener noreferrer">Vagrant</a> for provisioning vm. VirtualBox should be running before starting our cluster. First we’ll pull CoreOS vagrant image, enter into a directory and clone the image</p>
<pre class="language-bash"><code class="language-bash">$ <span class="token function">git</span> clone <span class="token punctuation">[</span>https://github.com/coreos/coreos-vagrant.git<span class="token punctuation">]</span><span class="token punctuation">(</span>https://github.com/coreos/coreos-vagrant.git<span class="token punctuation">)</span> coreos
$ <span class="token builtin class-name">cd</span> coreos</code></pre>
<p>Our cluster will have 1 master node and 3 worker node, so we need to run 4 vm. Open the Vagrantfile and find <code class="language-text">$num_instances</code> and makes it value 4. All the machine is configured to have 1GB of memory, so if you want to increase or decrease the memory change the value of <code class="language-text">vm_memory</code> variables.</p>
<p>Now start all the vm</p>
<pre class="language-bash"><code class="language-bash">$ vagrant up</code></pre>
<p>After sometimes all the vm will be started, check all the machine are running by below command</p>
<pre class="language-bash"><code class="language-bash">$ vagrant status</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*89e2LQOjSxbCaaJxoKOudg.png" alt="All 4 machine are running"></p>
<p>Now we’ll ssh into the first machine and start our swarm cluster</p>
<pre class="language-bash"><code class="language-bash">$ vagrant <span class="token function">ssh</span> core-01 -- -A</code></pre>
<p>To initialize the cluster we need the ip address of our machine. Run following command within the vm</p>
<pre class="language-bash"><code class="language-bash">$ <span class="token function">ip</span> addr show</code></pre>
<p>It’ll show a bunch of network related information. The information we’re interested are the ip address of eth1 interface.</p>
<p><img src="https://cdn-images-1.medium.com/max/2370/1*tt-qdwdl7FvaH-qpx-WPmA.png" alt="Finding IP of our machine"></p>
<p>Now we’ll start the cluster, run the following command</p>
<pre class="language-bash"><code class="language-bash">$ docker swarm init --advertise-addr node_ip_address</code></pre>
<p><code class="language-text">--advertise-addr</code> is used to tell docker swarm which ip address other node should use to join the cluster.</p>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*AXHsGDSvTTq-gtcAdeWTEA.png" alt="Initialize swarm cluster"></p>
<p>So the command started the cluster and made the current machine the manager. You can see it also printed a token which other will use to join the cluster. You should keep this token secret.</p>
<p>Now let’s add a worker node to the cluster. First ssh into the second vm</p>
<pre class="language-bash"><code class="language-bash">$ vagrant <span class="token function">ssh</span> core-02 -- -A</code></pre>
<p>And within the vm run following command</p>
<pre class="language-bash"><code class="language-bash">$ docker swarm <span class="token function">join</span> --token TOKEN manager_node_ip:2377</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*HLA800M7G8pwuoVZu9UvaQ.png" alt="Attaching a worker node to the cluster"></p>
<p>We can add the other vm as worker node in same way</p>
<p><img src="https://cdn-images-1.medium.com/max/2650/1*T9FFEV2PcDgl2ApB3k8f0g.png" alt="Adding 3 worker node to the cluster"></p>
<p>Now in the manager node we can see how many nodes are running by following command</p>
<pre class="language-bash"><code class="language-bash">$ docker node <span class="token function">ls</span></code></pre>
<p><img src="https://cdn-images-1.medium.com/max/2614/1*vX2-NWoAohES4BmPPvJg2Q.png" alt="List all the nodes"></p>
<p>We can see 4 nodes are running currently, three of them as worker and a manger node. That means we have successfully started a swarm cluster of 4 nodes on our local machine.</p>
<p>Hopefully this was helpful. Happy Coding!</p>
</div></div></div></main><footer class="pt-16 pb-4 flex justify-between"><div>
      Copyright © 2019 hasnayeen.dev. All rights reserved.
    </div><div><a href="/gh-page-test/about" class="text-indigo-700 font-semibold">Credits</a></div></footer></div>
    <script>window.__INITIAL_STATE__={"data":{"blogPost":{"id":"d1106e1e0feec8f82d0d95e15096418f","title":"Building a Swarm Cluster in Local Machine with CoreOS","date":"May 29, 2017","content":"\u003Cp\u003ESo last week I built a cluster on my local machine with docker swarm to get a taste of clustering. I used CoreOS as operating system on all the nodes as CoreOS is a container specific os. CoreOS also has several tools themselves for cluster orchestration. In this post I’ll show you how I made the cluster on vagrant machines.\u003C\u002Fp\u003E\n\u003Cp\u003EFirst of all you should have \u003Ca href=\"https:\u002F\u002Fwww.virtualbox.org\u002Fwiki\u002FDownloads\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EVirtualBox\u003C\u002Fa\u003E or \u003Ca href=\"https:\u002F\u002Fmy.vmware.com\u002Fweb\u002Fvmware\u002Fdownloads\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EVMware\u003C\u002Fa\u003E to run virtual machine on your host machine and you’ll also need \u003Ca href=\"https:\u002F\u002Fwww.vagrantup.com\u002Fdownloads.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EVagrant\u003C\u002Fa\u003E for provisioning vm. VirtualBox should be running before starting our cluster. First we’ll pull CoreOS vagrant image, enter into a directory and clone the image\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ \u003Cspan class=\"token function\"\u003Egit\u003C\u002Fspan\u003E clone \u003Cspan class=\"token punctuation\"\u003E[\u003C\u002Fspan\u003Ehttps:\u002F\u002Fgithub.com\u002Fcoreos\u002Fcoreos-vagrant.git\u003Cspan class=\"token punctuation\"\u003E]\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Ehttps:\u002F\u002Fgithub.com\u002Fcoreos\u002Fcoreos-vagrant.git\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E coreos\n$ \u003Cspan class=\"token builtin class-name\"\u003Ecd\u003C\u002Fspan\u003E coreos\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EOur cluster will have 1 master node and 3 worker node, so we need to run 4 vm. Open the Vagrantfile and find \u003Ccode class=\"language-text\"\u003E$num_instances\u003C\u002Fcode\u003E and makes it value 4. All the machine is configured to have 1GB of memory, so if you want to increase or decrease the memory change the value of \u003Ccode class=\"language-text\"\u003Evm_memory\u003C\u002Fcode\u003E variables.\u003C\u002Fp\u003E\n\u003Cp\u003ENow start all the vm\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ vagrant up\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAfter sometimes all the vm will be started, check all the machine are running by below command\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ vagrant status\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*89e2LQOjSxbCaaJxoKOudg.png\" alt=\"All 4 machine are running\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ENow we’ll ssh into the first machine and start our swarm cluster\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ vagrant \u003Cspan class=\"token function\"\u003Essh\u003C\u002Fspan\u003E core-01 -- -A\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ETo initialize the cluster we need the ip address of our machine. Run following command within the vm\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ \u003Cspan class=\"token function\"\u003Eip\u003C\u002Fspan\u003E addr show\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EIt’ll show a bunch of network related information. The information we’re interested are the ip address of eth1 interface.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2370\u002F1*tt-qdwdl7FvaH-qpx-WPmA.png\" alt=\"Finding IP of our machine\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ENow we’ll start the cluster, run the following command\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ docker swarm init --advertise-addr node_ip_address\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Ccode class=\"language-text\"\u003E--advertise-addr\u003C\u002Fcode\u003E is used to tell docker swarm which ip address other node should use to join the cluster.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*AXHsGDSvTTq-gtcAdeWTEA.png\" alt=\"Initialize swarm cluster\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESo the command started the cluster and made the current machine the manager. You can see it also printed a token which other will use to join the cluster. You should keep this token secret.\u003C\u002Fp\u003E\n\u003Cp\u003ENow let’s add a worker node to the cluster. First ssh into the second vm\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ vagrant \u003Cspan class=\"token function\"\u003Essh\u003C\u002Fspan\u003E core-02 -- -A\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAnd within the vm run following command\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ docker swarm \u003Cspan class=\"token function\"\u003Ejoin\u003C\u002Fspan\u003E --token TOKEN manager_node_ip:2377\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*HLA800M7G8pwuoVZu9UvaQ.png\" alt=\"Attaching a worker node to the cluster\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EWe can add the other vm as worker node in same way\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2650\u002F1*T9FFEV2PcDgl2ApB3k8f0g.png\" alt=\"Adding 3 worker node to the cluster\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ENow in the manager node we can see how many nodes are running by following command\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E$ docker node \u003Cspan class=\"token function\"\u003Els\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2614\u002F1*vX2-NWoAohES4BmPPvJg2Q.png\" alt=\"List all the nodes\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EWe can see 4 nodes are running currently, three of them as worker and a manger node. That means we have successfully started a swarm cluster of 4 nodes on our local machine.\u003C\u002Fp\u003E\n\u003Cp\u003EHopefully this was helpful. Happy Coding!\u003C\u002Fp\u003E\n"}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/gh-page-test/assets/js/app.ba0f1f39.js" defer></script><script src="/gh-page-test/assets/js/page--src--templates--blog-post-vue.32f2e99a.js" defer></script>
  </body>
</html>
